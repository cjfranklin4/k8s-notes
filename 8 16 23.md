# Todayâ€™s warmup
- https://github.com/csfeeser/k8s/blob/master/day1_basic_pod.md

# Lab 7 - Pods and the control pane
![[Pasted image 20230816094611.png]]
![[Pasted image 20230816094628.png]]
![[Pasted image 20230816094634.png]]
![[Pasted image 20230816095456.png]]
![[Pasted image 20230816095501.png]]
### etcd
- used to manage information on your nodes
- if this goes down your nodes go down

![[Pasted image 20230816095505.png]]
### controller
- controllers manage all the nodes in its cluster

![[Pasted image 20230816100347.png]]
### kubelet
- when the API server sends a request to node1, **kubelet is what listens for incoming commands from the controller**
- kubelet creates the pods


![[Pasted image 20230816095517.png]]![[Pasted image 20230816095521.png]]
![[Pasted image 20230816095527.png]]
![[Pasted image 20230816095531.png]]

#### get a wider output for pod info
```
kubectl get pods -o wide
```
![[Pasted image 20230816094852.png]]

#### edit pod
- will display a larger YAML manifest for your pod with all the information kubernetes has on how the pod currently exists in the clusrter
```
kubectl edit pod [podname] [-n] [namespace]
```

# Kubernetes Cert
- https://github.com/cncf/curriculum/blob/master/CKAD_Curriculum_v1.27.pdf

# namespaces
![[Pasted image 20230816104553.png]]

![[Pasted image 20230816104601.png]]

```
kubectl get namespaces

kubectl get ns
```

- create a namespace
```
kubectl create ns [namespace name]
```

- create pod wihtin a namespace
```
kubectl run [podname] --image
```

![[Pasted image 20230816104613.png]]

- get pods within a specieid namespace
```
kubectl get pods -n [namespace]
```

- create an object
```
kubectl create -f [yamlfile]
```

- create or change exisitgn object
```
kubectl apply -f [yamlfile]
```

- interative edits to the file
```
kubectl edit <resource> <obj>
```

- delete an object
```
kubectl delete -f object.yaml
```

# Lab 15 - creating and configuring namespaces
![[Pasted image 20230816105830.png]]

**What is a namespace?**  
_It is a slice of the cluster that can be limited by resource usage or connectivity to other namespaces. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces can_ **not** _be nested inside one another and each Kubernetes resource can only be in one namespace._

**What is a pod?**  
_A pod is an example of a resource you could build with Kubernetes. Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Containers are deployed within pods._

**How many namespaces do you see in your environment?**  
_By the conclusion of this lab; dev, test, prod, default, kube-system, kube-public, and kube-node-lease._

**What namespace(s) is set up when deploying Kubernetes?**  
_default, kube-system, kube-public, and kube-node-lease_

- <mark style="background: #ff9ed7;">To learn more about a specific namespace</mark>, try the command listed below. Replace _default_ with any of the other namespaces which are present from above.
```
kubectl describe ns [namespace]
```
```
##Output of the above
Name:         test
Labels:       <none>
Annotations:  <none>
Status:       Active

No resource quota.

No LimitRange resource.

```

- By default, _containers run with unbounded compute resources on a Kubernetes cluster_. 
- With **resource quotas**, <mark style="background: #ff9ed7;">cluster administrators can restrict resource consumption and creation on a namespace basis</mark>. 
	- Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota. There is a concern that one Pod or Container could monopolize all available resources. 
- A **LimitRange** is a <mark style="background: #ff9ed7;">policy to constrain resource allocations (to Pods or Containers) in a namespace.</mark>

- In order to create a new namespace called **test**, you need to apply a namespace YAML file to the cluster. Check out this YAML file. The contents of the file will be translated to JSON and sent to the API on the controller node.
```yaml
---
apiVersion: v1    # from kubernetes.io/docs
kind: Namespace   # type of resource to be created
metadata:
  name: test      # name to associate with this namespace
```

- create a namespace from a file
```
kubectl create -f ~/mycode/yaml/test-ns.yaml
```

- We also can create several different objects in Kubernetes purely from the command line. Create the **dev** namespace with the following command.
```
kubectl create ns dev
```

- Now let's create a `ResourceQuota` and **attach it to a new namespace**. Note the difference. Within Kubernetes, the value **spec** (short for 'specification') is a complete description of the desired state, including configuration settings provided by the user. For a complete description of this value, search for "spec and status" in the following document: [https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
```yaml
---
apiVersion: v1           # from kubernetes.io/docs
kind: ResourceQuota      # kubernetes resource to be created
metadata:
  name: tiny-rq          # name of ResourceQuota
spec:
  hard:                  # "may not exceed"
    cpu: "1"             # 1 cpu is == 1 vcore on a hypervisor or 1 hyperthread on bare metal
    memory: 1Gi          # 1 gig of memory
```

- Use **kubectl** to create a `ResourceQuota` object from the file **rq-tiny.yaml** and apply it to the namespace **test**.
```
kubectl apply -f ~/mycode/yaml/rq-tiny.yaml --namespace=test


kubectl apply -f ~/mycode/yaml/rq-tiny.yaml -n dev
```
```
##output of the above
Name:         test
Labels:       <none>
Annotations:  <none>
Status:       Active

Resource Quotas
 Name:     tiny-rq
 Resource  Used  Hard
 --------  ---   ---
 cpu       0     1
 memory    0     1Gi

No LimitRange resource.
```

- here is another ResourveQuota file
```yaml
---
apiVersion: v1           # from kubernetes.io/docs
kind: ResourceQuota      # kubernetes resource to be created
metadata:
  name: prod-resourcequota  # name of ResourceQuota
spec:
  hard:                  # "may not exceed"
    cpu: "2"             # 1 cpu is == 1 vcore on a hypervisor or 1 hyperthread on bare metal
    memory: 2Gi          # 2 gigs of memory
```

- `Describe` <mark style="background: #ff9ed7;">the namespaces to show that the new resource quotas have been created</mark>. Notice how we can 'daisychain' the namespaces **dev** and **prod** at the end of this command.
```
kubectl describe namespace dev prod
```

- delete resource quotas on a namespace
```
kubectl delete -f ~/mycode/yaml/rq-tiny.yaml --namespace=test
```

# Kubectl get and sorting
![[Pasted image 20230816111925.png]]

![[Pasted image 20230816111931.png]]

## Lab 17 will be Homework

# Kubectl port-forward
![[Pasted image 20230816112124.png]]
![[Pasted image 20230816112128.png]]

- create a port forward from your current location to the specified port
```
kubectl port-forward [pod-name] [local-port]:[container port]
```

# Lab 19 - kubectl port-forward
When you need to debug a service which resides inside a pod, then port-forwarding comes to the rescue. For instance, you've got a web server and a database that are connected, but now you want to run additional commands on the db to prototype new SQL queries. The Postgres port is not exposed except within the pod namespace. This is fine for our web server and being able to access it. But now you want to get your hands on the database that's running in order to run new SQL queries. You can port forward that container's Postgres port back to your machine (local host) so we can run psql cli or pgcli in order to poke and prod at the database directly.
![[Pasted image 20230816113026.png]]

- Let's begin by taking a quick look at what contexts are available to our kubectl client. <mark style="background: #ff9ed7;">Remember, a context is a reference to a cluster, namespace, and use</mark>r.
```
kubectl config get-contexts
```

- Set kubectl so it controls the namespace `default`, within our cluster, as the user `admin`. We gave this configuration the name, `kubernetes-the-alta3-way` context.
- <mark style="background: #ff9ed7;">set the content in k8s</mark>
```
kubectl config use-context kubernetes-the-alta3-way
```

- Run the `apply` command against the manifest to create the pod.
```
kubectl apply -f  ~/mycode/yaml/simpleservice.yaml
```

- According to the README on the simpleservice GitHub repo, a number of API endpoints should be available. The issue is that this image and its APIs will be running in a container. That container will be in a Pod with a random IP address, on some Node that also has it's own IP address. However, if we want to cut straight to that Pod, we can.

- In the new terminal, **create a tunnel from your local machine to your pod**.
```
kubectl port-forward simpleservice 2224:9876 --address=0.0.0.0
```

# Readiness and Liveness probes
### `httpGet` vs `exec`
- `httpGet` sends a GET request, but may not work for every contraimer if no API is installed
- `exec` uses command line to execute a command in the container and check for errors

- probes are added within a manifest
![[Pasted image 20230816120343.png]]
- l<mark style="background: #ff9ed7;">iveness probe</mark> **checks if the container is alive.** and checks for this every X seconds
- if the liveness probe sees the <mark style="background: #ff9ed7;">container is not running</mark>, **it will kill the container and restart it**

![[Pasted image 20230816120215.png]]
- If readiness probe fails, no traffic will be sent to the container

![[Pasted image 20230816120222.png]]

# Lab 23 - Implement Probes and Health Checks

This lab will check to see which nodes are ready.

Then this lab will teach how to configure containers to use LivenessProbes and ReadinessProbes and explore their use cases.

If a node isn't ready, there may be a problem with it. Nodes labeled as "ready" can accept pod deployment. However, if a node comes up as "not ready," there's the possibility of a problem... or the node simply hasn't finished initializing.

Similarly, containers can have **ReadinessProbes** inserted into them. This will help services determine if it should start to use the Pod or if it has to wait until all the containers inside of the Pod have become ready.

Also, **LivenessProbes** can be placed into containers as well. These are intended to assure that a container is still performing its essential tasks. If they are deemed not to be alive anymore, even though the container itself is still running, the LivenessProbe will cause a reboot of the container.

- Now it's time to work with a **livenessProbe**. This one in particular will cause k8s to send HTTP GET messages every 5 seconds to `http:<POD-IP>:9876/health`. **Create the following manifest** that will make use of our **si**mple**se**rvice image.
```
vim ~/sise-lp.yaml
```

- The following solution has a rather complex `spec.containers`. All available values for `spec.containers` are available at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core which may be worth reviewing before you create the following solution:
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: sise-lp
spec:
  containers:
  - name: sise
    image: mhausenblas/simpleservice:0.5.0
    ports:
    - containerPort: 9876
    livenessProbe:
      initialDelaySeconds: 2   # wait this long after restarting to do the first HTTP GET (default 0)
      periodSeconds: 5         # how often to probe (default is 1)
      timeoutSeconds: 1        # how long to wait until timeout occurs (default is 1)
      successThreshold: 1      # min consecutive successes to be considered "up" after a fail (default 1)
      failureThreshold: 3      # how many consecutive fails before pod is restarted (default 3)
      httpGet:                 # send HTTP GET to :9876/health/
        path: /health
        port: 9876
```

- View the liveness of your probe
```
kubectl describe pods sise-lp | grep -i health
```
```
kubectl port-forward sise-lp 2224:9876 --address=0.0.0.0
AND
curl localhost:2224/health
```


- Also, take a look at the k8s documentation for liveness and readiness probes. This link has all of the settings available: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes).

```yaml
---
apiVersion: v1
kind: Pod       # resource to create
metadata:
  name: badpod  # name of pod to create
spec:
  containers:
  - name: sise  # name of container to create
    image: mhausenblas/simpleservice:0.5.0   # image from hub.docker.com
    ports:
    - containerPort: 9876
    env:                # injecting ENV vars to the image simpleservice causes behavior changes
    - name: HEALTH_MIN  #  the min. and max. delay in milliseconds that the /health endpoint responds
      value: "2000"     # in ms (wait a min of 2 seconds before responding to GET)
    - name: HEALTH_MAX  #  the min. and max. delay in milliseconds that the /health endpoint responds
      value: "4000"     # in ms (wait a max of 4 seconds before responding to GET)
    livenessProbe:
      initialDelaySeconds: 2   # wait this long after restarting to do the first HTTP GET (default 0)
      periodSeconds: 5         # how often to probe (default is 1)
      timeoutSeconds: 1        # how long to wait until timeout occurs (default is 1)
      successThreshold: 1      # min consecutive successes to be considered "up" after a fail (default 1)
      failureThreshold: 3      # how many consecutive fails before pod is restarted (default 3)
      httpGet:                 # send HTTP GET to :9876/health/
        path: /health
        port: 9876
```
- The manifest is named `badpod`, because of the settings we are passing to the environmental variables `HEALTH_MIN` and `HEALTH_MAX`. 
- The values provided within the manifest will cause the service to respond to HTTP GET messages with (at least) a 2 second delay (and up to) a 4 second delay. 
- Note that in this manifest, we are also setting up a `livenessProbe`, to check on the health of the container every 5 seconds, expecting a response to the HTTP GET message within 1 second (smaller than the response time configured within simpleservice). **In short, this pod is setup to fail it's LivenessProbes.**

- After 2 seconds Kubernetes will send an HTTP GET to `:9876/health`. When this HTTP GET probe fails 3 consecutive times, Kubernetes will restart the pod. Run the following command every minute to observe this behavior. _Note: Watch the restart count on the pod failing the readiness check._

- Of course, a liveness probe on its own is not enough. We need more probes! **Readiness probes** <mark style="background: #ff9ed7;">describe when a container is ready to serve user requests</mark>. **Containers that fail readiness checks are assumed to be booting and are removed from service load balancers.** They are configured similarly to liveness probes.
```yaml
---
apiVersion: v1
kind: Pod             # create this kind of resource
metadata:
  name: sise-rp         # name of the Pod (siMPLE seRVICE rEADINESSpROBE)
spec:
  containers:
  - name: sise   # name of the container
    image: mhausenblas/simpleservice:0.5.0
    ports:
    - name: web
      protocol: TCP
      containerPort: 9876     # port the container is listening on
    readinessProbe:
      initialDelaySeconds: 10   # wait this long after restarting to do the first HTTP GET (default 0)
      periodSeconds: 3          # how often to probe (default is 1)
      timeoutSeconds: 2         # how long to wait until timeout occurs (default is 1)
      successThreshold: 1       # min consecutive successes to be considered "up" after a fail (default 1)
      failureThreshold: 3       # how many consecutive fails before pod is restarted (default 3)
      httpGet:                  # use HTTP GET to test :9876/health/
        path: /health
        port: 9876
```

>**Performing a Readiness Check**  
>Oftentimes when an application first starts up, it isn't ready to handle requests. There is usually some amount of initialization that can take up to several minutes. One thing the service object does is to track which of your pods are ready via a readiness check. 
>
>**Three successive failures of the readiness check will report the pod as "not ready."** 
>
>However, **if only one check succeeds**, then the pod will again be **considered ready.**

- another readiness probe manifest
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simpleservice2
  labels:
    name: simpleservice2
spec:
  containers:
  - name: simpleservice-web
    image: mhausenblas/simpleservice:0.5.0
    ports:
    - name: web
      containerPort: 9876
      protocol: TCP
    readinessProbe:
      periodSeconds: 2
      failureThreshold: 3
      successThreshold: 1
      httpGet:
        path: /health
        port: 9876
```

- when the readiness check fails, `kubectl get pods` will say `0/1` ready
![[Pasted image 20230816135511.png]]

- when liveness check fails, pod `restarts` count will keep going up\

# Lab 20 kubectl exec and cp
![[Pasted image 20230816144123.png]]

- to execute command inside a pod
```
kubectl exec [podname] -- [command]
```

- to get an interactive session
```
kubectl exec -it [podname] -- [command]
```

![[Pasted image 20230816144133.png]]
- copy files from Local to Pod
```
kubectl cp [local file path] <pod-name>:[path]
```

- copy files from POD to CONTAINER
```
kubectl cp <pod-name>:[path] [local file path]
```

- copy files to PODs with more than one container
```
kubectl cp [local file path] <pod-name>:[path] -c [container-name]
```

# Lab 21 - performing commands inside a pod
- Get into a container to cause a purposeful restart of the pod. To start, we'll use the `apply` command. The `apply` command allows a user to create or update resources. More on this later- for now, just know we're creating a pod resource. The **-f** flag is 'file', and it is fine that the file is accessed over https. there's no need to keep files local if there is a secure place to store them remotely.
```
kubectl create -f https://raw.githubusercontent.com/alta3/kubernetes-the-alta3-way/main/labs/yaml/nginx-pod.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.18.0
    ports:
    - containerPort: 80
```

- Now we can connect to a shell within the running container by using the `exec` command. The `sh` at the end of the command specifies that we wish to use the Bourne shell. If you get "error: unable to upgrade connection: container not found ('nginx')", wait a few seconds, as the pod is most likely still getting the container image launched.
```
kubectl exec -it nginx -- sh
```

- magine our container is running some really poorly written code, but it provides a critical service. The patch is in the works but for now we'll pretend that some spectacular failure occasionally happens. The take away is that if the container stops, Kubernetes should turn it back on. This is different than, say, KVM (or other hypervisors)- if we're 'inside' a Virtual Machine and shut it down, it will stay down. However, because Kubernetes focuses on service management, and shares the same kernel with the container, it's quite easy for Kubernetes to recognize the service has failed and turn it back on.

- You issue `kill -15 1`, which should end all processes you're 'allowed' to shut down, including PID 1 (you've been terminated).
```
kill -15 1
```

- Kubernetes focuses on service management so let's check on the NGINX pod. It seems that as fast as you killed the pod, Kubernetes _**restarted**_ it. Even the old pod is cleaned up. <mark style="background: #ff9ed7;">You issue a command to sort the pods by restart count</mark>. You notice that the NGINX container is still running, although the RESTARTS column has been incremented by 1.
```bash
kubectl get pods --all-namespaces --sort-by='.status.containerStatuses[0].restartCount'
```
```
#output
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-5d65dd49c8-jmgcj                   1/1     Running   0          36h
kube-system   calico-kube-controllers-7798c85854-dbfvh   1/1     Running   0          36h
kube-system   calico-node-8thjn                          1/1     Running   0          36h
kube-system   calico-node-hfcj8                          1/1     Running   0          36h
kube-system   calico-node-jrqrg                          1/1     Running   0          36h
kube-system   coredns-5d65dd49c8-67rgd                   1/1     Running   0          36h
default       nginx                                      1/1     Running   1          8m1s

```

# restartPolicy
![[Pasted image 20230816150405.png]]

# Pod security context
- <mark style="background: #ff9ed7;">security context</mark> is an **optional set of params** that can be attached to **EITHER the entire CONTAINER or a SINGLE POD to establish a rule**

![[Pasted image 20230816150516.png]]
![[Pasted image 20230816150521.png]]


![[Pasted image 20230816150526.png]]

# Lab 25 - applying security contexts
- A Pod or Container may have a <mark style="background: #ff9ed7;">SecurityContext</mark> **applied to it in order to define privileges and access parameters.**

- It is possible to set all of the containers inside of the Pod to have the same privileges and access parameters by setting the `PodSecurityContext`. To do this, you can add a `securityContext` field inside of the Pod specifications. See this truncated example of where to put the securityContext.

```
spec:
  securityContext:
    runAsUser: 1000 # specifies all processes run with user ID 1000
    runAsGroup: 3000 # specifies all processes run with primary group ID 3000
    fsGroup: 2000 # specifies all processes run with secondary group ID 2000

```

- <mark style="background: #ff9ed7;">Notice how it is at the same level that you would define the containers at</mark>. 
- We also will be able to **set a securityContext inside of any individual container**. That would look like this:
```
containers:
- name: secured-container
  image: mhausenblas/simpleservice
  securityContext:
    runAsUser: 2000
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true

```

- In Linux, every user has a number assigned to them for tracking purposes (UIDs). The same is true for groups (GIDs). If you wanted to see those assignments, you can (provided you have proper permissions) by displaying the contents of `/etc/passwd`

- The **first way we are going to secure our pod** is to ensure that we are **forcing anybody accessing our cluster to run as a specific user**, **by providing their User ID**. 
	- Here we are setting the UID to be 1000, 
	- as well as disabling the ability to run as a privileged (root) user. 
	- Finally, we'll make the root file system read only. 
- Create the following Pod manifest:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secured-cont
spec:
  securityContext:
    fsGroup: 2000
  containers:
  - name: secured-container
    image: busybox:1.34.0
    command: [ "sh", "-c", "sleep 1h" ]
    securityContext:
      runAsUser: 1000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
    volumeMounts:
    - name: secured-volume
      mountPath: /data/secured
  volumes:
    - name: secured-volume
      emptyDir: {}
```

- Now, let's exec inside of the Container to examine what privileges we have.
```
kubectl exec -it secured-cont -- sh
```
- Once inside of your shell, take a look at the processes you are privy to. The output from this command should show that the USER is 1000, just like we had in our **runAsUser** field of the securityContext.
```
PID   USER     TIME  COMMAND
    1 1000      0:00 sleep 1h
   12 1000      0:00 sh
   17 1000      0:00 ps
```

- Next, take a look inside of the /data directory. You should only have the `/secured` directory. You will notice that it has the group ID of 2000, which we set while using our fsGroup.
```
total 4
drwxrwsrwx    2 root     2000          4096 Feb 18 21:53 secured
```

- Inspect the file. This will show you that the user ID is 1000 (set by the **runAsUser** field), and the group ID of 2000 (set by the **fsGroup** field).
```
total 4
-rw-r--r--    1 1000     2000            18 Feb 18 21:59 security-test.txt
```

- **To assure that this pod does not allow for any root access, the group ID (gid) needs to not be 0**. 
- Although we have set the **fsGroup** to 2000, if we would ignore the **runAsGroup** field, the process still would be able to interact with root group files. That is why we set the **runAsGroup** to 3000. 
- Verify that this is the case by running the following command.
```
id
```
```
uid=1000 gid=3000 groups=2000
```

# Resource management
- kubectl top
	- display resource level (CPU/Memory usage etc)
	- for example: nodes, pods, 
```
kubectl top [type] [--all-namespaces]
```

# Limits, Requests, and Namespace ResourceQuotas
![[Pasted image 20230816155939.png]]
- `requests` - used to gurantee the min amount of resources to allocate to a container


![[Pasted image 20230816155943.png]]
- `limit` - caps out the resources allocated to a container

![[Pasted image 20230816155948.png]]
- you will want to put requests/limits on a container if 

![[Pasted image 20230816155954.png]]

# Lab 27 - Defining Resource Requirements, Limits and quotas

With Kubernetes, a pod requests the resources required to run its containers. Kubernetes guarantees that these resources are available to the pod. Most common are CPU and memory, but GPU is also available.  
We want to create another YAML file in order to accomplish the goal of this lab. This YAML file is actually our pod manifest which will serve to cap resource limits on a pod.

- Define the pod with the minimum required resources for where it is allowed to land when deployed. To do this, create a pod manifest.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: linux-pod-r
spec:
  containers:
  - name: ubuntu-r
    image: ubuntu:jammy
    command: ["/bin/bash", "-ec", "while :; do echo '.'; sleep 5 ; done"]
    resources:
      requests:
        cpu: "300m"
        memory: 256Mi
    ports:
    - containerPort: 80
```

- Take a look at the resources you have given the Pod.
```
cpu:        300m
memory:     256Mi
```
> **Capping Resource Usage**  
> In addition to setting the resources required by a pod, which establishes the minimum resources available to the pod, a maximum may be set on a pod's resource consumption via resource limits.

- here is another pod manifest with more limits
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: linux-pod-rl
spec:
  containers:
  - name: ubuntu-rl
    image: ubuntu:jammy
    command: ["/bin/bash", "-ec", "while :; do echo '.'; sleep 5 ; done"]
    resources:
      requests:
        cpu: "300m"
        memory: 256Mi
      limits:
        cpu: "300m"
        memory: 512Mi
    ports:
    - containerPort: 80
```

- Let's use the kubectl `describe` command to find out if our pod has the requested resources and limitations.
```
kubectl describe pods linux-pod-rl | egrep Limits -A 2

kubectl describe pods linux-pod-rl | egrep Requests -A 2
```

- Update the apt cache inside the Linux pod. First we will need a DNS server set in the config.
```
kubectl exec -it linux-pod-rl -- bash

echo "nameserver 8.8.8.8" | tee /etc/resolv.conf

exit

kubectl exec -it linux-pod-rl -- apt-get update
```

- Test the limits controls by loading the server and watching CPU and memory utilization. Install `htop` inside the linux-pod-rl pod.
```
kubectl exec -it linux-pod-rl -- apt-get install htop -y
```

- Next install `stress` inside the pod.
```
kubectl exec -it linux-pod-rl -- apt-get install stress
```

# Lab 28 - Kubectl Top and Application Monitoring
In this lab we'll be setting up a Metrics Server in our new cluster. A Metrics Server is a cluster-wide aggregator of resource usage data. It collects metrics like CPU or memory consumption for containers or nodes, from the Summary API, exposed by Kubelet on each node.

We will confirm success by using `top`.

- Make sure that it is working as expected by issuing `toping` command. If you get the response `error: metrics not available yet`, that simply means that the metrics server is still gathering the information that it needs. Wait for about 30 to 90 seconds and then try again. **Within a few minutes, it should begin working**
```
kubectl top nodes
```
```
NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
node-1   71m          3%     1765Mi          46%       
node-2   80m          4%     1417Mi          37% 
```

- Next, top the pods.
```
kubectl top pods --all-namespaces
```
```
NAMESPACE     NAME                                       CPU(cores)   MEMORY(bytes)
kube-system   calico-kube-controllers-659fb845bc-xzbf8   2m           16Mi
kube-system   calico-node-8ct5q                          35m          82Mi
kube-system   calico-node-swgr7                          27m          82Mi
kube-system   kube-dns-688c69f57d-dbxcp                  3m           18Mi
kube-system   metrics-server-847dcc659d-qxcwc            5m           15Mi
```