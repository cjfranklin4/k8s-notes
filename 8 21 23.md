# DaemonSet
- makes all pods run on a all nodes (one on each node) (to ensure pods are balanced)

- use deployments if you have more replicas than nodes
	- but you can also specify what node you want a pod to go on for deployments now
- .

- daemonsets are only used for INTERNAL SERVICES on your clusters

![[Pasted image 20230821102929.png]]

### Controlling DaemonSets with Node selectors and labels
![[Pasted image 20230821102936.png]]


# Services
- you can `curl` into yout container by curling the port-forward url

- by default pods CAN talk to each other, BUT they cannot find each other by name (so, they can’t communicate)
	- Pods DONT keep their IP addresses, 

### Service
- a <mark style="background: #FF5582A6;">service</mark> is a **kubernetes object that acts as a a big NEON ARROW constantly pointing to tthe location of pods within your cluster**
	- can be associated with a deployment and the replicaSet within the deployment

- ip address of the service vs ip address of the pod
![[Pasted image 20230821105614.png]]
![[Pasted image 20230821105642.png]]
![[Pasted image 20230821105656.png]]

- Pods NEED a specfici label to help be exposed and selected by the service
![[Pasted image 20230821111009.png]]

# Blye/Green and Canary Deployment strategies
- 

![[Pasted image 20230821111353.png]]
![[Pasted image 20230821111400.png]]
![[Pasted image 20230821111405.png]]
![[Pasted image 20230821111410.png]]
![[Pasted image 20230821111415.png]]
![[Pasted image 20230821111419.png]]
![[Pasted image 20230821111424.png]]

# Lab 62 - Advanced Deployment Strategies
Along with the **Recreate** and **RollingUpdate** strategies, it is also worth exploring two more advanced deployment strategies: **Blue/Green** and **Canary** deployments.

- lets create a blue deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bluegreenapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bluegreen
      version: v1.0
  template:
    metadata:
      labels:
        app: bluegreen
        version: v1.0
    spec:
      containers:
      - name: container01
        image: hashicorp/http-echo:alpine
        args:
        - '-text="Welcome to the Blue App (1.0)!"'
```

- Next **we will create a ClusterIP service** for access to our _blue_ deployment.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: bluegreen
spec:
  ports:
  - port: 5678
    protocol: TCP
    targetPort: 5678
  selector:
    app: bluegreen
    version: v1.0
  sessionAffinity: None
  type: ClusterIP
```

- Now inspect the endpoints that the service is keeping track of.
```
kubectl get endpoints bluegreen -o yaml | grep name:
```
```
name: bluegreen
    name: bluegreenapp-67886f8667-8f76h
    name: bluegreenapp-67886f8667-hm57x
    name: bluegreenapp-67886f8667-9lvlm
```

- Now, let's create a new version of this app, the **Green** version.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: greenblueapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: greenblue
      version: v2.0
  template:
    metadata:
      labels:
        app: greenblue
        version: v2.0
    spec:
      containers:
      - name: container01
        image: hashicorp/http-echo:alpine
        args:
        - '-text="Welcome to the Green App (2.0)!"'
```

- start up the new deployment
```
kubectl apply -f green.yml
```

- view all deployments
```
kubectl get deploy
```

- Let's update the service so that it is looking at our other deployment. Make sure it looks exactly like the below.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: bluegreen
spec:
  ports:
  - port: 5678
    protocol: TCP
    targetPort: 5678
  selector:
    app: greenblue
    version: v2.0
  sessionAffinity: None
  type: ClusterIP
```

- apply the changes to the service
```
kubectl apply -f bluegreensvc.yml
```

- reinspect the endpoints of the service
```
kubectl get endpoints bluegreen -o yaml | grep name:
```
```
name: bluegreen
    name: greenblueapp-b7595fc7b-5wkgk
    name: greenblueapp-b7595fc7b-wfr4m
    name: greenblueapp-b7595fc7b-dfzjp
```

- delete deployments
```
kubectl delete deploy --all
```

## Canary deployments
Canary Deployments allow us to once again **have two parallel instances of the deployments** running, **but only direct a portion of the traffic coming into our service towards the new version of the deployment.** Then, when the new **Canary** version of the deployment has been adequately tested, you can simply increase the percent of traffic going to the newer version.

- create a new deployment called exisitingapp
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: existingapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: yellow
      version: v1.0
  template:
    metadata:
      labels:
        app: yellow
        version: v1.0
    spec:
      containers:
      - name: container01
        image: hashicorp/http-echo:alpine
        args:
        - '-text="Welcome to the Existing App (1.0)!"'
```

- lets make a version 2 of the above deployment called canaryapp
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: canaryapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: yellow
      version: v2.0
  template:
    metadata:
      labels:
        app: yellow
        version: v2.0
    spec:
      containers:
      - name: container01
        image: hashicorp/http-echo:alpine
        args:
        - '-text="Welcome to the Canary App (1.0)!"'
```

- start both deployments
```
kubectl apply -f existingapp.yml -f canaryapp.yml
```

- Now we need to create a service that will be able to direct traffic to both of these deployments. Notice that this time, our service **will not select by version**.
	- IT SELECTS BY LABEL
```yaml
apiVersion: v1
kind: Service
metadata:
  name: yellowbird
spec:
  ports:
  - port: 5678
    protocol: TCP
    targetPort: 5678
  selector:
    app: yellow #SELECTING BY THE LABEL YELLOW
  sessionAffinity: None
  type: ClusterIP
```

- view details of a service
```
kubectl describe svc yellowbird
```

- inspect the endpoints for the new service
```
kubectl get endpoints yellowbird -o yaml | grep name:
```
```
name: yellowbird
  name: existingapp-556878c678-pdr5g
  name: existingapp-556878c678-bvhbf
  name: existingapp-556878c678-jp9zr
  name: canaryapp-7948d6dcc5-22ccn
```

> Here we see that 25% of our traffic will be flowing to the _canaryapp_ version, while 75% remains flowing to the older _existingapp_ deployment.

- Let's create a fun script to actually see how our data will be split between the **existingapp**, and the new **canaryapp**!
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: testpine
  namespace: default
spec:
  containers:
  - image: alpine:3.2
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent
    name: alpine
  restartPolicy: Always
```

- create the new testing pod

- run the following commands to set up curl on the pod
```
CLUSTERIP=`kubectl get svc yellowbird --template '{{.spec.clusterIP}}'`
kubectl exec -it testpine -- apk update
kubectl exec -it testpine -- apk upgrade
kubectl exec -it testpine -- apk add curl
```

- Use testpine as the source Pod to test connectivity to the yellowbird service. You will see either Canary or Existingapp response but not both at the same time.
```
kubectl exec -it testpine -- curl http://$CLUSTERIP:5678
```

- Let's repeat that previous step 20 times. Do you notice the destination changes randomly?
```
for i in {1..20}; do kubectl exec -it testpine -- curl http://$CLUSTERIP:5678; done
```
```
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"     
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Canary App (1.0)!"                                                           
"Welcome to the Existing App (1.0)!"                                                         
"Welcome to the Existing App (1.0)!" 
```

- Now that we are ready to move forward with our next version of our app, we will simply cause all of the traffic to flow to the new version through our built in ability to scale.

- We will first scale up the new version, and then scale down the old version of the deployment to zero.
```
kubectl scale deploy canaryapp --replicas 4 && kubectl scale deploy existingapp --replicas 0
```

- Let's see how that has affected which endpoints our service is keeping track of.
```
kubectl get endpoints yellowbird -o yaml | grep name:
```
```
name: yellowbird
    name: canaryapp-7948d6dcc5-mdmrh
    name: canaryapp-7948d6dcc5-8swth
    name: canaryapp-7948d6dcc5-gdlld
    name: canaryapp-7948d6dcc5-22ccn
```

> 100% of traffic is on the v2.0 app!


# Lab 64 - Horizontal Pod Autoscaler
In this lab, you will create a deployment that will autoscale according to CPU and memory utilization. This time, however, we're going to leave figuring out every task and procedure to create the HPA Autoscaler up to you. Though answers, and hints will be available, this is an excellent opportunity to test your knowledge of Kubernetes, and finely tune the HPA and the Containers for the traffic created by our Load Generator.

HorizontalPodAutoscalers (HPAs) will scale your cluster up within a very short period of time, as soon as the load limit for the current number of replicas (`targetAverageUtilization`) is passed. However, the people behind Kubernetes were smart enough to not have it scale down at the same rate. If they had, this would have caused "thrashing", which means that your Cluster would be resizing faster than it is able to launch or delete the Pods. You can read some more about the HPA algorithm here: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details).

> **IMPORTANT** -- Prior to beginning this lab, be sure all Pods, Deployments, resourceQuotas, and Limitranges are deleted from the default namespace. Furthermore, before beginning, close any additional TMUX panels you might have (Type exit and press enter to close a TMUX panel).

- Create the following manifest, called `sise-deploy-scaled.yaml`. This will be the target for our HPA.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sise-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sise
  template:
    metadata:
      labels:
        app: sise
    spec:
      containers:
      - name: sise
        image: mhausenblas/simpleservice:0.5.0
        ports:
        - containerPort: 9876
        env:
        - name: SIMPLE_SERVICE_VERSION
          value: "1.0"
        resources:
          requests:
            cpu: '15m'
            memory: '16Mi'
          limits:
            cpu: '30m'
            memory: '32Mi'
```

- launch the deployments

- Launch the Kubernetes Metrics Server so we can 'top' our **nodes** and **pods**.
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

- Next, we'll create a **ClusterIP** Service for our **sise deployment**.
```
kubectl expose deploy sise-deploy
```

> By using the `expose` command, we are creating a ClusterIP service for our deployment. If you would like more information about your service, try using the following command: `kubectl describe service sise-deploy`

- et's use the linux command `watch` to keep an eye on our deployment.
```
watch -d 'kubectl get pods'
```

- Launch linux `watch` to analyze the Metrics Server's output.
```
watch -d 'kubectl top pods'
```

- We have one last watch to setup to help us finely tune the Horizontal Pod Autoscaler. But, we'll have to split our top panel vertically to give us space. Once we create our HPA, we'll use this panel to view Resource Utilization in comparison to our thresholds.

- Create the following manifest for your Horizontal Pod Autoscaler:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sise-deploy # keep an eye on this deployment for scaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sise-deploy
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60 #each pod's container should use this percentage of resources on average. if it gets higher, create a new pod
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 60
```

### [Beginning Procedure](https://live.alta3.com/content/kubernetes/labs/content/kubernetes/scaling/autoscaling-challenge.html#beginning-procedure)

Now that we are all setup, it is time for us to begin the actual lab. This part of the lab is mostly un-guided, however hints, tips, and solutions are available below to help you along. **Your task is to appropriately tune the HPA**. This will require changes to the sise-deploy-scaled.yaml manifest, and the sise-hpa.yaml manifest (answer keys available below). In order to determine how best to scale the **HPA** you're going to watch all of the data come in as we

- launch the HPA
```
kubectl apply -f sise-hpa.yaml
```

- Let's start up a `watch` command for `kubectl get hpa`. This is an excellent method to visually watch our metrics as the HPA is making scaling decisions. This will enable us to appropriately tune our HPA and Deployment to give us the desired results.
```
watch -d 'kubectl get hpa'
```

- Now, analyze the deployment. Once the HPA launched, it should have scaled the deployment to **3** replicas, as specified in the HPA Manifest. Take a moment and analyze the deployment to see if there are any scaling abnormalities. Give it at least two to three minutes.
# Network Policy


![[Pasted image 20230821104200.png]]
![[Pasted image 20230821104205.png]]
![[Pasted image 20230821104209.png]]
![[Pasted image 20230821104224.png]]
![[Pasted image 20230821104229.png]]

### Examples of connectivity types
![[Pasted image 20230821104234.png]]
![[Pasted image 20230821104240.png]]

### how to write networkpolicy s
- network policies are like resourceQuotas, you attach them to namespaces
![[Pasted image 20230821134607.png]]

- `podSelector: {}` - empty curly brace means all pods in the current namespace
- `policyTypes` - a list of types of traffic the policy is handling
	- can be both `Ingress`, `Egress`, one or the other, or none
![[Pasted image 20230821134857.png]]

### Example of a DEFAULT DENY network policy
- STOP all POD to POD traffic in a namespace
- because there are NO SET RULES about what rules are allowed for Ingress or Egress, it means NO COMMUNICATION/TRAFFIC is allowed
![[Pasted image 20230821135107.png]]

### Example network policy - traffic rules based on labels
![[Pasted image 20230821135715.png]]
- going to choose specific pods based on labels
![[Pasted image 20230821140336.png]]


OR
![[Pasted image 20230821140307.png]]

### matchExpressions
- can bse used to select pods with either of a list of labels
- also if you have two very different labels like "run: nginx" and "slappy: mcgoo" you would write that like this:
	- **both expressions being listed are "or" statements**
```yaml
matchExpressions: 
   - {key: run, operator: In, values: [nginx]}
   - {key: slappy, operator: In, values: [mcgoo]}
```
![[Pasted image 20230821141013.png]]
- 
### matchLabels
- if you add multiple labels under this, it only selects pods THAT HAVE ALL THE LISTED LABELS
- if you want “OR” a label, use matchExpressions

### Ingress and Egress rules - with namespaces as well 
![[Pasted image 20230821143541.png]]
![[Pasted image 20230821143814.png]]

- egress: only allowed to send traffic/send requests to 
![[Pasted image 20230821143908.png]]
![[Pasted image 20230821144041.png]]



# Lab 71 - NetworkPolciy Basics
Network Policies help to give a Kubernetes Administrator control over the intricate Pod intranetworking. This is incredibly useful in assuring that your cluster is secure and cannot be attacked.

Some examples include:

- Only allow traffic between pods that form part of the same application. For example, in a frontend-backend application, only allow communication to the backend from frontend pods.
- Isolate pod traffic in namespaces. That is, a pod can only communicate with pods that belong to the same namespace.

Further Reading:

- [https://kubernetes.io/docs/concepts/services-networking/network-policies/#prerequisites](https://kubernetes.io/docs/concepts/services-networking/network-policies/#prerequisites)
- [https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)
- [https://docs.bitnami.com/kubernetes/how-to/secure-your-kubernetes-application-with-networkpolicies/](https://docs.bitnami.com/kubernetes/how-to/secure-your-kubernetes-application-with-networkpolicies/)

- Create a pod manifest to deploy `client`. If you never worked with **BusyBox** then you should take a few minutes to read about the project: https://www.busybox.net/about.html
```yaml
---
kind: Pod   # create a Pod
apiVersion: v1
metadata:
  name: client   # name of pod
  labels:
    run: client   # <-- run: client is a key/value applied to this machine
spec:
  containers:
  - name: busybox
    image: busybox:1.34.0    # https://www.busybox.net/about.html
    resources:
      requests:
        cpu: "300m"   # slice of core
        memory: 128Mi   # Mb of RAM
    args:
    - sleep
    - "10000"
```

- start up the client pod
```
kubectl create -f client.yaml
```

- Now, let's create another client Pod. Name this one **client02** and give the label **run: client02**.
```yaml
---
kind: Pod   # create a Pod
apiVersion: v1
metadata:
  name: client02   # name of pod
  labels:
    run: client02
spec:
  containers:
  - name: busybox
    image: busybox:1.34.0    # https://www.busybox.net/about.html
    resources:
      requests:
        cpu: "300m"   # slice of core
        memory: 128Mi   # Mb of RAM
    args:
    - sleep
    - "10000"
```

- Create a pod by using the kubectl run command. Remember, the earlier labs showed how to fix this issue with an extra option.
```
kubectl run nginx --image=nginx:1.19.6
```

- Expose the nginx Pod through a service using the kubectl expose command:
```
kubectl expose pod nginx --port=80
```

- - Once the `client.yaml` file is applied, you should be able to access the pod and receive back the NGINX welcome page using the following command.
```
kubectl exec client  -- wget -T 2 -q nginx -O -
```

- ow we will create a **default deny** policy. This will allow us stop communication between pods.
```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: netpol-1
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

> Let's take a closer look at this manifest. Notice how we are using **{}** for our Pod Selector. 
> This will select **EVERY** pod in our namespace. 
> 
> Then, we specify both the **Ingress** and **Egress** policy types, meaning we are instantiating an Ingress/Egress Network Policy on every pod in the **default** namespace. 
> 
> Network Policy, once enacted, works much like **RBAC** -- **THERE ARE NO DENY RULES**. Network Policies are additive. Therefore, this Network Policy serves as a Default Deny across the entire namespace.

- Next, let's create this network policy.
```
kubectl apply -f default-deny-np.yaml
```

- Test connectivity between pods. This time it fails, as expected!
```
kubectl exec client  -- wget -T 2 -q nginx -O -
```

- Now we will create a network policy which will **allow only the client pod to connect with nginx**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: netpol-1
spec:
  podSelector: 
    matchLabels:
      run: nginx
  ingress:
  - from:
    - podSelector:
        matchLabels:
          run: client
  policyTypes:
  - Ingress
```

- Let's go ahead and apply the new manifest. Since the two manifest share the same name, the existing Network Policy will be reconfigured.
```
kubectl apply -f client-network-policy.yaml
```

# Types of services
- ClusterIP - most common type of service
![[Pasted image 20230821151112.png]]

# Networking with Services
![[Pasted image 20230821151417.png]]
- Nodes also have IP addresses
- Node snd Service IP addresses DO NOT CHANGE

## Types of services
- 3 types:
- ClusterIP: 
- NodePort
- LoadBalancer
![[Pasted image 20230821152442.png]]
![[Pasted image 20230821153829.png]]


![[Pasted image 20230821151422.png]]
## Load balancing'
- kube-proxy is a component that lives on the node
- when an incoming request comes in, by default, kube-proxy chooses a random pod
![[Pasted image 20230821151426.png]]

### NodePort
- is a number thats always benwteen 30000-32767
	- it is unique
	- is tied to a service in your cluster ip subnet
![[Pasted image 20230821153952.png]]
![[Pasted image 20230821154137.png]]






![[Pasted image 20230821151431.png]]

![[Pasted image 20230821151435.png]]

![[Pasted image 20230821151443.png]]

![[Pasted image 20230821151448.png]]


# Lab 74 - Expose Applications via Services
- Create a service which the service discovery of Kubernetes can look up.
- Allow the application to find and talk with the Kubernetes API.
![[Pasted image 20230821154737.png]]

Just like a Fully Qualified Domain Name (FQDN) is simply a named pointer to a specific IP address at a given time, a Kubernetes Service is a pointer to one or more Pod IP addresses.

All Services terminate at a Pod IP address, otherwise known as an EndPoint. It follows then, that if your service appears 'up', but it is not mapped to an Endpoint IP, chances are the Pod(s) 'down'!

These are the three primary types of Services that you need to be familiar with:

- ClusterIP Service
- NodePort Service
- LoadBalancer Service

#### [1 - ClusterIP Service](https://live.alta3.com/content/kubernetes/labs/content/kubernetes/networking/Exposing-a-Service.html#1---clusterip-service)

A ClusterIP service can only be accessed from inside of the cluster. This means that you, as a Kubernetes Administrator, have the ability to access all of the ClusterIP services via kubectl.

This is accessible via the **spec.clusterIp** port. If a spec.ports[*].targetPort is set it will route from the port to the targetPort. The CLUSTER-IP you get when calling kubectl get services is the IP assigned to this service within the cluster internally.

#### [2 - NodePort Service](https://live.alta3.com/content/kubernetes/labs/content/kubernetes/networking/Exposing-a-Service.html#2---nodeport-service)

A NodePort Service simply exposes a specific port on **every node in your cluster**, and any traffic that hits that port will then be directed into an EndPoint (Pod) via iptables or netfilter rules.

Your NodeIPs are the external IP addresses of the nodes. If you access this service on a nodePort from the node's external IP, it will route the request to spec.clusterIp:spec.ports[_].port, which will in turn route it to your spec.ports[_].targetPort, if set.

Here are two diagrams to help you see what may happen. If it is not clear to you how a nodePort works, please ask your instructor.

_Please Note: These "Single endpoints" represent Pods residing inside of the Node that is pointing to it_

**Access via node-2 Client to endpoint flow.**
```
                   +---------+
                   | client  |
                   +--+---^--+
                     1|   |6
+---------+   2    +--v---+--+
|         |  <---  |         |
|  node 1 |  SNAT  |  node 2 |
|         |  --->  |         |
+---+-^---+   5    +---------+
  3 | | 4
+---v-+---+
| Single  |
|endpoint |
+---------+

```
**Access via node1 Client to endpoint flow**
```
                        +---------+
                        | client  |
                  1     +---^--^--+
         +------------------+  |
No SNAT  |  +-------------------
         |  |      4
     +---+--+--+        +---------+
     |  node 1 |        |  node 2 |
     +---+-^---+        +---------+
       2 | | 3
     +---v-+---+
     | Single  |
     |endpoint |
     +---------+
```

#### [3 - LoadBalancer Service](https://live.alta3.com/content/kubernetes/labs/content/kubernetes/networking/Exposing-a-Service.html#3---loadbalancer-service)

Most likely, you do not want to be exposing _n_ number of Node IP addresses as Public IP addresses. Most cloud providers, such as GKE and AWS, have built load balancers into their cloud that a Kubernetes **LoadBalancer** service is able to dynamically configure.

A **LoadBalancer** service will assure Pods are ready, and then notify the Cloud Load Balancer of the route to the specific Pods. You can access this service from your load balancer's IP address, which routes your request to a nodePort, which in turn routes the request to the ClusterIP (this then load balances to the pods providing that service). You can access this service as you would a NodePort or a ClusterIP service as well.

Envoy (written by Lyft) is typically the load balancer of choice. Unfortunately, Kubernetes has poor support for other External Load Balancers (or maybe other external load balancers have poor support for Kubernetes), so unless you're using Envoy, it is likely you will need to create your own "glue code" to connect the **load Balancer** service to your own Load Balancer.

If you are interested in learning more about Services, check out these other resources:

- **Exposing External IP addresses** - [https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/](https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/)
- **Basics of Exposing Services** - [https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)
- **Understanding Services** - [https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/](https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/)


- lets create an nginx conf file
```nginx
user  nginx;
worker_processes  1;
error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;
events {
    worker_connections  1024;
}
http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
    access_log  /var/log/nginx/access.log  main;
    sendfile        on;
    keepalive_timeout  65;
    server {
        server_name  localhost;
        listen 80;
        root /var/www;

        location / {
          sendfile  on;
          index index.html;
        }
    }
}

```

- Now, recreate an updated `nginx-conf` configmap
```
kubectl create configmap nginx-conf --from-file=nginx.conf
```

> What is a configMap? We want this new configuration file to be 'injected' into our pod at the time it is created. This behavior takes the place of creating a new image with the proper configuration file in place, as well as manually copying the new file into the pod. First create a **ConfigMap** that includes your configuration data, then describe the config map within the manifest.

Once again, you made this file in a previous lab, but let's make sure you didn't make any errors in creating it. Run the following command to create `index.html`. Replace any content with what is shown in the below code block.
```html
<!DOCTYPE html>
<html>
<head>
<title>K8s</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to K8s!</h1>
<p>There is a mailbox here. To the north is a house.</p>
<p>Open Mailbox</p>
<p/There is a leaflet inside</p>
<p>take leaflet</p>
<p>Taken</p>
<p>Read leaflet</p>
<p>Welcome to the land of Kubernetes.</p>
<p>No cloud should be without one!</p>

<p><em>Thank you for using K8s.</em></p>
<a href="/static/">Click to see static files (once they are configured)</a>
<a href="/webby/"> Click to see webby (once it is configured)</a>
</body>
</html>

```

- Create a ConfigMap for the `index.html` file. This is being created so that the file may be automatically injected into a pod at the time of creation.
```
kubectl create configmap index-file --from-file=index.html
```

- view all config maps
```
kubectl get cm
```

- Create your `nginx-conf`
```yaml
---
apiVersion: v1   # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#_v1_podspec
kind: Pod                      # MUST - create a Pod resource
metadata:                      # MUST - metadata.name provides the 'name' for the resource
  name: nginx-configured       # MUST - name of the resource
  labels:                      # OPTIONAL (best practice) - allows manipulation of resource by label
    app: nginx-configured      # OPTIONAL - example of a key: value key pair
spec:                          # MUST - what you want to build
  containers:                  # MUST - describe 1 or more containers within the pod
  - name: nginx                # MUST - name of the container within the pod
    image: nginx:1.18.0         # MUST - image nginx and version 1.18.0 from hub.docker.com
    ports:                     # exposes ports on the pod
    - name: chris              # name of mapped port (could also be Tim, or pinecone)
      containerPort: 80        # exposed port
    volumeMounts:                     # describes where your volumes will be available within the container
    - name: nginx-proxy-config        # matches spec.volumes[0].name
      mountPath: /etc/nginx/nginx.conf  # this is WHERE it will appear inside of the container
      subPath: nginx.conf             # required to overwrite a file that is already within the container
    - name: my-index-file             # matches spec.volumes[1].name
      mountPath: /var/www/index.html
      subPath: index.html
    - name: static-demo-data          # matches spec.volumes[2].name
      mountPath: /var/www/static/nginx.txt
      subPath: nginx.txt
  volumes:  
  - name: nginx-proxy-config      # matches spec.volumeMounts[0].name
    configMap:
      name: nginx-conf            # name of a configMap
  - name: my-index-file           # matches spec.volumeMounts[1].name
    configMap:
      name: index-file            # name of a configMap
  - name: static-demo-data        # matches spec.volumeMounts[2].name
    configMap:
      name: nginx-txt             # name of a configMap
```

- expose a pod
```
kubectl expose pod/nginx-configured --type="NodePort" --port 80
```

- Confirm that the new service has been added to the list of services.
```
kubectl get services | grep nginx
```

- Run the describe services command to see what the output looks like.
```
kubectl describe services/nginx-configured
```
```
Name:                     nginx-configured
Namespace:                default
Labels:                   app=nginx-configured
Annotations:              <none>
Selector:                 app=nginx-configured
Type:                     NodePort
IP:                       172.16.3.203
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32118/TCP
Endpoints:                192.168.2.64:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
```

- Get your test pod's node port and store it in an environmental variable called `NODE_PORT`.
```
export NODE_PORT=$(kubectl get services/nginx-configured -o go-template='{{(index .spec.ports 0).nodePort}}')
```

- For informational purposes, determine what NODE `nginx-configured` is running on.
```bash
kubectl describe pod nginx-configured | grep "Node:\|IP:"
```

1. **Question: What node is your pod actually running on?**
    - _Answer: `kubectl get pods -o wide` or `kubectl describe pods` can help you find those._
2. **Question: If you curl the service's NodePort on the nodes that are not hosting the pod, will you STILL get a response?**
    - _Answer: Yes- the last few lab directions show you what happens._
3. **Question: What is the IP of each node? (node-1, node-2)?**
    - _Answer: `kubectl describe nodes | grep IP` should do the trick!_
4. **Question: If you ping node-1 and node-2 does it resolve to the expected IP address?**
    - _Answer: It should resolve correctly._
5. **Question: What TCP socket would you curl to test if you can access your service at node-1?**
    - _Answer: The one that is listed as the NodePort._
6. **Question: What TCP socket would you curl to test if you can access your service at node-2?**
    - _Answer: Same as answer 5._

- The following command will curl node-1 on your service's nodeport.
```bash
curl node-1:$NODE_PORT/static/nginx.txt
```

> If the above command fails, see if your pod is running with "`kubectl get pods`". If the pod is failing to come up, delete it with "`kubectl delete -f nginx-configured.yaml`". 
> 
> Next, check to ensure your ConfigMaps are in place with "`kubectl get cm`". If you do not have all 3 configmaps, or if they were made in correctly, your Pod will fail to boot. 
> 
> To fix, try deleting all of your ConfigMaps, recreating them, and then recreating your pod.

# LoadBalancer service
- a loadBalancer is a seperate object
- ![[Pasted image 20230821163341.png]]
- all traffic goes through the loadBalancer then to your nodes

- but K8s does not include loadBalancers,, you have to bring your own