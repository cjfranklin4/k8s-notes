# Troubleshooting 2
https://github.com/csfeeser/k8s/blob/master/troubleshooting_part2.md

# Scale
```
kubectl scale deploy [deployment name] -n [namespace] --replicas=[num of replicas]
```

# Annotations
- are similar to labels
- they are key value pairs, and go in the `metadata` section of a manifest
- annotations are JUST metadata, they **only store information about a specific object**, but have nothing to do with how the object runs
### view annotations
```
kubectl describe [type] [name]
```

```
kubectl edit [type] [name]
#and view them in the manifest
```


![[Pasted image 20230822102236.png]]


![[Pasted image 20230822102243.png]]

![[Pasted image 20230822102249.png]]

# Lab 53 - Add an annotatiom
Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. They are a way for other programs driving Kubernetes via an API to store some opaque data with an object. Annotations can be used for the tool itself or to pass configuration information between external systems. Defining an annotation is done within the metadata section in a Kubernetes object.

You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.

- here is a pod with an annotation
```yaml
---
apiVersion: v1          # API version (endpoint) to use
kind: Pod               # this is the resource to create
metadata:
  name: nginx-annot     # this is the name of the kubernetes resource
  annotations:          # this is an annotation
    alta3.com/logo-url: "https://static.alta3.com/images/Alta3-logo_large.png"      
spec:
  containers:
  - name: nginx         # name of the container
    image: nginx:1.18.0  # image and version of image to use
    ports:
    - containerPort: 80
```

- Deploy the pod
```
kubectl apply -f ~/nginx-annot.yaml
```

- Verify the annotation using kubectl.
```
kubectl describe pod nginx-annot | grep Annotations
```
```
Annotations:      alta3.com/logo-url: https://static.alta3.com/images/Alta3-logo_large.png
```

> If you decided to add another annotation to your manifest and ran the above command again, you will **not** see the second annotation on the list because the term "Annotations" which you are grepping for only exists once in the description of the pod. 
> 
> **Instead you will run the same command with an added flag, _-A 1_ to get and additional line after the initial match on "Annotations".** 
> 
> Command would look like so. `kubectl describe pod nginx-annot | grep -A 1 Annotations`. You can increase that number the more annotations you have.

# Flow of traffic in pod to pod communication
### internal pod to pod -  kube-proxy
![[Pasted image 20230822103555.png]]

### communicate from outside cluster - nodePort
- nodeport points to nodepoint service, then follows the kube-proxy flow
![[Pasted image 20230822103757.png]]
![[Pasted image 20230822103843.png]]

### load balancer service
- it allows k8s to use a loadbalancer
- load Balancer is an external ip address
- load balancer service will let you configure things more precsely than kube-proxy
![[Pasted image 20230822104051.png]]

# Ingress
![[Pasted image 20230822104543.png]]
![[Pasted image 20230822104623.png]]

- ingress - an object/manifest you can write in K8s `kind: Ingress`
- ingress is used to allow us to make more rules and regulation on traffic and how our services are accessed
- <mark style="background: #FF5582A6;">NEEDS TO BE BUILT ON TOP OF A LOAD BALANCER</mark>



# Ingress Controllers
- .

![[Pasted image 20230822103258.png]]

## Ingress controllers
- made by 3rd party, but you need to install the ingress controller before you can use the ingress object in k8s
- 

![[Pasted image 20230822103303.png]]

### ingress manifest
- you NEED annotations 
- **setting annotations on ingress can be used** to <mark style="background: #ff9ed7;">CONFIGURE how that ingress object works</mark>
- .

![[Pasted image 20230822103308.png]]
![[Pasted image 20230822105647.png]]
![[Pasted image 20230822110011.png]]


![[Pasted image 20230822103313.png]]

![[Pasted image 20230822103317.png]]

![[Pasted image 20230822103322.png]]

![[Pasted image 20230822103328.png]]

# Lab 77 - explose applications via ingress controllers
Create Services containing the endpoints of Pods. Create Ingress resource to expose Services.

In Kubernetes, an Ingress is a special type of service that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which other services within the cluster.

This lets you consolidate your routing rules into a single resource. For example, you might want to send requests to example.com/api/v1/ to an api-v1 service, and requests to example.com/api/v2/ to the api-v2 service. With an Ingress, you can easily set this up without creating multiple LoadBalancers or NodePorts. Both LoadBalancers and NodePorts can represent finite resources and costs depending on the hosting and network providers of your Kubernetes cluster.

NodePort and LoadBalancer let you expose a service by specifying that value in the service’s type. Ingress, on the other hand, is a completely independent resource to your service. You can declare, create, modify, and destroy it separately to your services. This makes it decoupled and isolated from the services you want to expose. It also helps you to consolidate routing rules into one place.

One downside is that you will need to configure an Ingress Controller for your cluster. But that’s pretty easy— in this example, we’ll use the Nginx Ingress Controller.
![[Pasted image 20230822110641.png]]

- Installing the Nginx Ingress Controller may vary depending on the provider that you are using (AWS, GCE-GKE, Azure... read about them here: [https://kubernetes.github.io/ingress-nginx/deploy/](https://kubernetes.github.io/ingress-nginx/deploy/).
```yaml
#GENERATED FOR K8S 1.20
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  name: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx
  namespace: ingress-nginx
...... (there is more but it ia not useful)
```

- deploy the new namesoace
```
kubectl apply -f ingress-deploy.yaml
```
```
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
```

- Let's confirm the new services running on our cluster.
```
kubectl get services -n ingress-nginx
```
![[Pasted image 20230822110945.png]]

- Let's confirm that the ingress controller pods are running:
```
kubectl get pods -n ingress-nginx
```
![[Pasted image 20230822111022.png]]

- Let's design a deployment named **nginx-ingress-demo** which uses an **httpd** image.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: demo
  name: demo-ingress
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: demo
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - image: docker.io/library/httpd:2.4.56
        name: httpd
        ports:
        - containerPort: 80
          protocol: TCP
```

- confirm the deployment is running
```
kubectl get deployment --show-labels
```
```
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
demo-ingress   1/1     1            1           20s   app=demo
```

- Create a service called demo-svc that **will expose (target) any deployment with the label** `app: demo`. 
	- **Indeed, this will target the deployment you just ran.**
```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: demo
  name: demo-svc
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: demo
  type: ClusterIP
```
> This specification will create a Service which targets TCP port 80 on any Pod with the **app:demo** label, and expose it on an abstracted Service port where
> 
> - **targetPort:** is the destination port this service will target which must be the container's listening port.
> - **port:** is the service's listening port, where any packets received here are immediately relayed to the targetPort

- Confirm that demo-service is running:
```
kubectl get services demo-svc
```
```
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
demo-svc       ClusterIP   172.16.xxx.yyy   <none>        80/TCP    45m
```

- Confirm that the demo-svc has targeted the demo-deployment's pod.
> As long as you see a destination IP address here, you are good to go. Continue with the next step.
```
kubectl describe services demo-svc | grep Endpoints
```

- Discover the FQDN of your lab environment with this linux-one-liner:
> The variable BASEURL will now contain: "aux1-" + your beachhead FQDN + ".live.alta3.com/"
```
export BASEURL="aux1-$(hostname -d).live.alta3.com"
```

- **Write your ingress manifest**. Just cut and paste the ontent below. No need to edit that `{{ BASEURL }}` jinja variable! You will use j2 in the next step to insert the BASEURL into this manifest. Note that you are creating a **j2** file type!
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: demo-ingress
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: {{ BASEURL }}
    http:
      paths:
      - backend:
          service:
            name: demo-svc
            port:
              number: 80
        path: / 
        pathType: Prefix
```

- Run j2 to replace the jinja variable {{ BASEURL }} with its storaed value.
```
j2 demo-ingress.j2  -o demo-ingress.yaml
```

- Check out the ingress object you just created.
```
kubectl describe ingress demo-ingress
```
![[Pasted image 20230822111651.png]]

- Discover the **HTTPS NodePort** of your **ingress-nginx-controller**. Since HTTPS runs on port **443**, you must discover the 443 node port.
```
kubectl get service ingress-nginx-controller -n ingress-nginx
```
![[Pasted image 20230822111843.png]]

- While the above is interesting if you are doing everything manually, but it would be difficult to automate, so try this command:
- <mark style="background: #ff9ed7;">THIS LETS US FIND THE NODE PORT INGRESS IS EXPOSED ON</mark>
```
kubectl get service ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.ports[*]}{"\n"}' | jq
```
```json
{
  "appProtocol": "http",
  "name": "http",
  "nodePort": 32503,
  "port": 80,
  "protocol": "TCP",
  "targetPort": "http"
}
{
  "appProtocol": "https",
  "name": "https",   <- Aha!  The https service
  "nodePort": 32649, <--- Therefore, you want this nodePort
  "port": 443,
  "protocol": "TCP",
  "targetPort": "https"
}

```

- Configure your load balancer to route traffic to our ingress controller usng HTTPS. To do this we'll set up a reverse proxy on your beachhead that will service inbound requests to the **aux1** student path.
```nginx
# Create a group of targets called "k8nodes"
upstream k8nodes {
  server node-1:NODE_PORT; # <-- REPLACE NODE_PORT with the HTTPS nodePort
  server node-2:NODE_PORT; # <-- REPLACE NODE_PORT with the HTTPS nodePort
}

server {

  listen 2224 default_server;
  listen [::]:2224 default_server;

  location / {
    # map the entire root path to "k8nodes"
    proxy_pass https://k8nodes;  # IMPORTANT, this selects HTTPS protocol
    proxy_set_header Host $http_host;
  }
}
```

> Notice that you are configuring a load balancer that is outside of the K8s cluster, therefore:
> 
> - **nodePorts** are used to speficy the next hop towards the target
> - **HTTPS** is the specified protocol, so only HTTPS nodePorts will work

> Kubernetes will NEVER help you with this step. In a k8s environment, you need to "BYOLB", (Bring Your Own Load Balancer) which explains why you must complete LB configuration. Since you already have a load balancer running on your bchd server, you will configure that load balancer to route traffic to your k8s ingress service.

- Finally, we need to test and reload the local host's nginx configuration.
```
sudo nginx -t

sudo nginx -s reload
```

# Storage
- containers are stateless
- if you need data to persist in a container, it must be kept OUTSIDE the container
	- configmaps
	- secrets - to keep tokens/passwords
	- **both of the above are stored in `etcd` - in the controller**
- but to store things like databases/etc?
	- needs to be stored on the node (physical machine where the clusters are stored)
- a new type of object connects a pod to a database on the node

- k8s loves breaking things into many diff pieces
- 

![[Pasted image 20230822121341.png]]

## Persistent Volume
- created by the cloud provider (for example)
- Persistent Volume Claim - grabs a portion of storage from the persistent volume
	- this is mounted as a volumem to an arbitrary directory within the container
	- ![[Pasted image 20230822133843.png]]
	- ![[Pasted image 20230822134418.png]]
	- both the pods are mounted to same pvc
- Persistent volumes can be STATIC or DYNAMIC
	- the above PV is static (choose a specific space size at first), or choose to have the size change dynamically
	- 


![[Pasted image 20230822121346.png]]

![[Pasted image 20230822121350.png]]

### storage class
![[Pasted image 20230822121354.png]]

![[Pasted image 20230822121358.png]]

# Lab 43 - Using PersistentVolumeClaims for Storage
Create a persistent volume using a local storage class.

This is NOT RECOMMENDED FOR PRODUCTION! This is an extremely simple way to create a persistent volume in a lab environment. As long as the host node itself is using persistent storage, then this method will inherit the persistent storage of the hosting node. In order for this to work, we need to use a `nodeSelector` to ensure that when a pod is deleted or redeployed, that pod lands on the same node where the data will be stored. Take note that there is no need to have a "local" storage class as the node itself is providing that service.

Further Reading: [https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes-insecure.html](https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes-insecure.html)

- Create the YAML file for the **PersistentVolume**.
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: alta3-pv
  labels:
    type: local
spec:
  storageClassName: manual #manual is used for local storage only on the node
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

- Now, create the **PersistentVolumeClaim Manifest**.
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

- Now create the **Pod Manifest** that includes the `volume.persistentVolumeClaim`.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-pv
spec:
  containers:
  - name: nginx-with-pv
    image: nginx:1.18.0
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
    volumeMounts:
    - name: nginx-pv-storage
      mountPath: "/data"
  nodeSelector:
    kubernetes.io/hostname: node-1 # force pod onto a specific node, where the pv is located
  volumes:
  - name: nginx-pv-storage
    persistentVolumeClaim: # mounting the pvc we created earlier
      claimName: nginx-pvc
```

- lets add a file to the PV, then delete and recreate the pod

### diagram of the lab
![[Pasted image 20230822145600.png]]

![[Pasted image 20230822145716.png]]

# Jobs and Cronjobs

### creating pods
![[Pasted image 20230822151514.png]]
![[Pasted image 20230822151532.png]]
![[Pasted image 20230822151551.png]]


![[Pasted image 20230822151045.png]]

![[Pasted image 20230822151050.png]]

![[Pasted image 20230822151054.png]]
- parallelism - how manu pods will work at the same time
- completions - how many times do the pods need to sucessfully complete
	- ![[Pasted image 20230822151942.png]]
- .

### cronjob
- an object that makes jobs
- allows you to set them according to a schedule
- 
![[Pasted image 20230822152804.png]]

# Lab 66 - Running and executing a job

1. How to write a Job manifest. Show similarities and differences with the Pod manifest.
2. Referencing logs to determine Job outcome.
3. What happens when we run a Job without a command.
4. Show a command that ends. Use sleep and watch commands to make this visibly clear.
5. Show a command that never ends.
6. Show how to automatically clean up after a job completes.
7. Use parallelism and completions to control resource consumption and processing time.

Oftentimes during our day, it becomes necessary to get a job done quickly. One approach is to slice the job into smaller tasks, then assign a team of workers to do it all at the same time. This is what a Kubernetes job does for us.

- create a job manifest
```yaml
apiVersion: batch/v1
kind: Job 
metadata:
  name: echocomplete
spec: 
  template:
    metadata:
      name: echocomplete
    spec:
      containers:
      - name: echo
        image: alpine:3.2
        command: ["/bin/sh"]
        args: ["-c", "echo hello; sleep 10"]
      restartPolicy: Never
```

- apply the job manifest
```
kubectl apply -f echocomplete.yaml
```
- Observe the watch we started in the bottom panel. You should see the pods created (they should last for 10 seconds based off the manifest we used).

- Now let's observe the logs that were created by that completed job. Replace **XXXXX** with the identifier of the pod. **Tip**: Use **tab** to autocomplete the pod name to save time.
```
kubectl logs echocomplete-XXXXX
```

- view all jobs
```
kubectl get jobs
```

- Let's see what happens if we run this job without a command.
```yaml
apiVersion: batch/v1
kind: Job 
metadata:
  name: echocompletenocommand
spec: 
  template:
    metadata:
      name: echocompletenocommand
    spec:
      containers:
      - name: echo
        image: alpine:3.2
        ###command: ["/bin/sh"]
        ###args: ["-c", "echo hello; sleep 10"]
      restartPolicy: Never
```
- after applying the job: Wow! It just ended. Well that actually makes sense because we did not tell the job to anything.

- let’s create a job that never ends
```yaml
apiVersion: batch/v1
kind: Job 
metadata:
  name: echocompleteneverends
spec:
  template:
    metadata:
      name: echocompleteneverends
    spec:
      containers:
      - name: echo
        image: alpine:3.2
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo This is the job that never ends. Yes it goes on and on my friends! Somebody started running it not knowing what it was, and they continued running it forever just because, ; sleep 10; done"]
      restartPolicy: Never
```

- In this example we will complete six jobs, running two of the six jobs at a time. **We do this by adding the keys, `parallelism` and `completions`.** Parallelism defines the number of jobs which can run at a time. Completions indicates the total number of times the job will be completed.
```yaml
apiVersion: batch/v1
kind: Job 
metadata:
  name: echocompletefantasia
spec:
  parallelism: 2
  completions: 6  
  template:
    metadata:
      name: echocompletefantasia
    spec:
      containers:
      - name: echo
        image: alpine:3.2
        command: ["/bin/sh"]
        args: ["-c", "echo hello; sleep 10"]            
      restartPolicy: Never
```
- Take note of the two new lines, **parallelism: 2** and **completions: 6**. The parallelism line will throttle job completion rate to run only two pods at a time. This will be repeated three times to meet the **completions: 6** requirement.

# Lab 67 - Scheduling a CronJob
1. Write a simple **cronjob**.
2. Observe that the **Job Template** definition is identical in both jobs and cronjobs.
3. Learn when and how to use **Starting Deadline**
4. Learn when and how to use **Concurrency Policy**
5. Learn when and how to use **Suspend**
6. Learn how to apply **Jobs History Limits**

https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#creating-a-cron-job

- In this lab, we're going to create a Job that will run on a schedule. In order to see satisfying results, we're going to schedule a job that will run at every minute. Create the following CronJob manifest.
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: echocomplete
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      parallelism: 1
      completions: 1
      template:
        spec:
          containers:
          - name: echo
            image: alpine:3.2
            command: ["/bin/sh"]
            args: ["-c", "echo hello; sleep 10"]
          restartPolicy: Never
```

- **Stay a while and listen (or watch, in this case)**. In the TMUX pane you created at the beginning of this lab, we have a watch command running. At every minute, the CronJob will spin up a single Pod. It is simply going to echo a phrase, and then sleep for 10 seconds. After 10 seconds, the pod will have the status **Completed**.
```
NAME                          READY   STATUS      RESTARTS   AGE
echocomplete-27998907-jxv2h   0/1     Completed   0          2m59s
echocomplete-27998908-7qfgg   0/1     Completed   0          119s
echocomplete-27998909-twcxj   0/1     Completed   0          59s
```

> As our CronJob runs, it is leaving Pod entries in Kubernetes. 
> 
> Now, the Containers are no longer running, but the ephemeral resources (such as log files) of the Pod are still being maintained by the Host. **In order to release those resources, we'll have to clean out the Pods.** 
> 
> **CronJobs will be cleaned out automatically, and this can be configured.** 
> 
> However, we can manually clean them out by either deleting the Pods directly, or the CronJob which created them.

- delete a cronjob
```
kubectl delete cj echocomplete
```
> Take a look at your watch panel! All of the completed Pods have been deleted. **When you delete a job, all of the pods will be deleted as well.**

- Let's take a deep dive into Concurrency Policies! 
- **ConcurrencyPolicy sets a rule only affecting Jobs run by the same manifest.** 
- If the set of tasks is still being worked on, then 3 options can be chosen for how the CronJob proceeds with a new scheduled task. 
	- **Allow** means that jobs can be run concurrently. This is the default. 
	- **Forbid** means **that if the old set of tasks has not been completed when new tasks are scheduled to run, the new tasks are skipped** until the old are completed. 
	- **Replace** is the opposite. **If the old tasks are not finished they will be stopped so the new tasks can begin**.
- .
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: concurrencypolicyechocomplete
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Allow
  jobTemplate:
    spec:
      parallelism: 1
      completions: 1
      template:
        spec:
          containers:
          - name: echo
            image: alpine:3.2
            command: ["/bin/sh"]
            args: ["-c", "echo hello; sleep 120"]
          restartPolicy: Never
```
> As you'll observe, new jobs will continue to be created, despite the previous ones still running. This is the default behavior, with the ConcurrencyPolicy being set to **Allow**. For these next couple of steps, feel free to simply read along, but if you'd like, change the manifest as well. This can just be a touch time consuming due to the nature of CronJobs.

- The above manifest were run with `concurrenyPolicy: Allowed` how many Pods will be running after 4 minutes, showing minute-by-minute analysis? If you watch for 4 minutes you will notice something interesting happen to the oldest completed Pod. View the Job History Limit section further below to understand what occurred.
```
Answer: 0 minutes, 1 Pod running
        1 minute,  2 Pods running
        2 minutes, 2 Pods running 1 Pod completed
        3 minutes, 2 Pods running 2 Pods completed
        4 minutes, 2 Pods running 3 Pods completed
```

- If the above manifest were running with `concurrenyPolicy: Forbid` how many Pods will be running after 4 minutes, showing minute-by-minute analysis?
```
Answer: 0 minutes, 1 Pod running
        1 minute,  1 Pod running
        2 minutes, 1 Pod running 1 Pod  completed
        3 minutes, 1 Pod running 1 Pod  completed
        4 minutes, 1 Pod running 2 Pods completed
```

- f the above manifest were running with `concurrenyPolicy: Replace` how many Pods will be running after 4 minutes, showing minute-by-minute analysis?
```
Answer: 0 minutes, 1 Pod running
        1 minute,  1 Pod running 1 Pod Terminating
        2 minutes, 1 Pod running 1 Pod Terminating
        3 minutes, 1 Pod running 1 Pod Terminating
        4 minutes, 1 Pod running 1 Pod Terminating
```

- Next, we'll learn when and how to use **Suspend**.
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: suspendechocomplete
spec:
  schedule: "*/1 * * * *"
  suspend: true
  jobTemplate:
    spec:
      parallelism: 1
      completions: 1
      template:
        spec:
          containers:
          - name: echo
            image: alpine:3.2
            command: ["/bin/sh"]
            args: ["-c", "echo hello; sleep 10"]
          restartPolicy: Never
```

> When the `suspend` key is set to `true` **all subsequent executions of the job are suspended until `suspend` is changed to false**. 
> 
> Tasks that are scheduled to run while the CronJob is still suspended are considered missed jobs. 
> 
> If `suspend` is patched to be changed to false, **then all missed jobs will immediately begin execution (provided there is no starting deadline).**

- Since the above manifest is running with `suspend: true` how many Pods will be running after 4 minutes, showing minute-by-minute analysis?
```
Answer: 0 minutes, 0 Pod running
        1 minute,  0 Pod running
        2 minutes, 0 Pod running 
        3 minutes, 0 Pod running 
        4 minutes, 0 Pod running 

```

- Run a quick **kubectl get** command and check out the status of this CronJob.
```
kubectl get cj suspendechocomplete
```
> Since this CronJob was created in a **Suspended** state, the jobs will not run until patched/edited to set **spec.suspend=false**.

- Finally, we'll discuss how to use `Jobs History Limits` to clean up completed Pods.
	- Job history limits can be set together or separately. `successfulJobsHistoryLimit` will keep around a specified number of completed jobs, default is 3. 
	- `failedJobsHistoryLimit` will keep around a specified number of failed jobs, if any. Default is 1. Create one last manifest.
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: historylimitechocomplete
spec:
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 1
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      parallelism: 1
      completions: 1
      template:
        spec:
          containers:
          - name: echo
            image: alpine:3.2
            command: ["/bin/sh"]
            args: ["-c", "echo hello; sleep 10"]
          restartPolicy: Never
```

- If the above manifest were running with the set history limits how many Pods will be shown as completed after 4 minutes, showing minute-by-minute analysis? If you watch close enough after 2 minutes you will see something interesting. Notice how only two Pods are retained in the **Completed** state (whereas the default setting would retain 3). All older pods will be deleted, and automatically cleaned up.
```
Answer: 0 minutes, 1 Pod running
        1 minute,  1 Pod running 1 Pod completed
        2 minutes, 1 Pod running 2 Pods completed 
        3 minutes, 1 Pod running 2 Pods completed
        4 minutes, 1 Pod running 2 Pods completed

```

# Helm
matm